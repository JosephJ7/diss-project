# ğŸ›´â€¯Mobyâ€¯Dublin â€“ Telemetryâ€¯Platform

> **An endâ€‘toâ€‘end data & analytics stack** that ingests realâ€‘time scooter telemetry from the Moby fleet in Dublin, processes it with Apacheâ€¯Spark, schedules jobs with Dagster, stores aggregates as Parquet on AWSâ€¯S3 and in MongoDB, and surfaces insights through a Streamlit dashboard.

<!-- <p align="center">
  <img width="80%" src="docs/architecture.svg" alt="Platform architecture"/>
</p> -->


## Tableâ€¯ofâ€¯Contents

1. [Features](#features)
2. [Architecture](#architecture)
3. [QuickÂ Start](#quick-start)
4. [ProjectÂ Structure](#project-structure)
5. [Configuration](#configuration)
6. [RunningÂ theÂ Pipeline](#running-the-pipeline)
7. [DashboardÂ Screens](#dashboard-screens)
8. [License](#license)


## Features

| Layer | Highlights |
|-------|------------|
| **Ingestion** | Streaming telemetry â†’ **S3 raw zone** via Spark AutoÂ Loader. |
| **Processing** | Incremental PySpark assets compute KPIs: batteryâ€‘decay, demand hotspots (H3), idleâ€‘bike alerts. |
| **Orchestration** | Dagster assets with hourly/daily sensors; local runs or CI triggers. |
| **Storage** | Parquet in AWSâ€¯S3 **and** MongoDB collections for fast dashboard reads. |
| **Visualization** | Streamlit app with 3 tabs (map, lineÂ chart, alert table). |
| **CI/CD** | Preâ€‘commit, Ruff, Black; GitHubÂ Actions placeholder for unit tests. |

### TechÂ Stack

- **Apacheâ€¯SparkÂ 3.4** â€“ distributed processing (local or YARN)
- **PythonÂ 3.10** â€“ language for ETL, orchestration, and dashboard
- **DagsterÂ 1.x** â€“ declarative orchestration & asset lineage
- **MongoDBÂ 7** â€“ lowâ€‘latency serving layer for Streamlit
- **AWSâ€¯S3** â€“ scalable object store for raw & processed **Parquet**
- **StreamlitÂ 1.33** (+â€¯PyDeck / Plotly) â€“ interactive telemetry dashboard

## Architecture

```mermaid
flowchart TD
  subgraph Ingestion
    A[Kafka / CSV] --> B[S3 raw zone]
  end
  subgraph Processing
    B --> C[PySpark silver]
    C --> D1[Batteryâ€‘decay asset]
    C --> D2[H3 demand asset]
    C --> D3[Idleâ€‘alert asset]
  end
  subgraph Serving
    D1 & D2 & D3 -->|Mongo sync| M[(MongoDB)]
    D1 & D2 & D3 -->|Parquet files| S3[(AWSÂ S3)]
  end
  M & S3 --> E[Streamlit dashboard]
```

## QuickÂ Start

> Prerequisites: **PythonÂ 3.10**, a running **MongoDB**, **SparkÂ 3.4** (local), and optional AWS credentials if you want to copy Parquet to S3.

```bash
# 1) Clone & set up virtualenv
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2) Configure environment (or copy .env.example)
export MONGO_URI="mongodb://localhost:27017/moby"
export AWS_ACCESS_KEY_ID=â€¦          # only if writing to S3
export AWS_SECRET_ACCESS_KEY=â€¦
export MAPBOX_TOKEN="pk.xxx"        # optional, nicer map tiles

# 3) Materialise an asset (example: demand hotspots)
poetry run dagster job run -m moby_pipeline.assets -j h3_demand_job

# 4) Launch Dagster UI (optional)
poetry run dagster dev  # http://localhost:3000

# 5) Start the dashboard
streamlit run dashboard.py          # http://localhost:8501
```


## ProjectÂ Structure

```
â”œâ”€â”€ moby_pipeline/          # Spark transforms & Dagster assets
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ assets.py           # KPI asset definitions (batteryâ€‘decay, H3 demand, idle alerts)
â”‚Â Â  â”œâ”€â”€ spark_utils.py      # helper functions / window specs
â”‚Â Â  â””â”€â”€ config.py           # shared constants (MAX_RANGE_M, etc.)
â”œâ”€â”€ dashboard.py            # Streamlit UI (3 tabs)
â”œâ”€â”€ docs/                   # SVG diagrams & screenshots
â”œâ”€â”€ requirements.txt        # Python deps
â”œâ”€â”€ .env.example            # Template for environment vars
â””â”€â”€ tests/                  # Unit tests
```


## Configuration

| Variable | Purpose |
|----------|---------|
| `MONGO_URI` | Mongo connection string (`mongodb://host:port/db`). |
| `AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY` | Credentials for writing Parquet to S3. |
| `MAPBOX_TOKEN` | Mapbox key for highâ€‘resolution tiles (optional). |
| `SPARK_MASTER` | Override Spark master URL (defaults to `local[*]`). |

Environment variables are loaded at runtime via **pythonâ€‘dotenv** â€“ copy `.env.example` to `.env` and edit.


## RunningÂ theÂ Pipeline

Using **Dagster**:

```bash
# Materialise a single asset
poetry run dagster job run -m moby_pipeline.assets -j battery_decay_job

# Or launch Dagster UI
poetry run dagster dev
```

Each asset writes:

| Asset | S3 path (Parquet) | MongoDB collection |
|-------|-------------------|--------------------|
| Batteryâ€‘decay | `s3://â€¦/battery_decay/` | `battery_decay` |
| H3 demand | `s3://â€¦/h3_demand/` | `h3_demand` |
| Idle alerts | `s3://â€¦/idle_alerts/` | `idle_alerts` |

## DashboardÂ Screens

| Tab | Visualisation | Data source |
|-----|---------------|-------------|
| **Demand map** | Hexâ€‘map (`pydeck.H3HexagonLayer`) | `h3_demand` |
| **Battery decay** | Line chart (Plotly) | `battery_decay` |
| **Idle alerts** | Data table | `idle_alerts` |

Screenshots live in **docs/**.

## License

Released under the **MIT License**. See [LICENSE](LICENSE) for full text.


## ğŸ™‹â€â™‚ï¸ Author

- **Joseph J.** â€“ [GitHub Profile](https://github.com/JosephJ7)


## ğŸ“¬ Contact

For feedback, issues, or suggestions:  
ğŸ“§ josephjacobie2001@gmail.com  
ğŸ“ Or create an [issue](https://github.com/JosephJ7/crimedetection-AYS/issues)